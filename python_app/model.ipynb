{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "df = pd.read_csv(\"loan_approval_dataset_normalized.csv\")\n",
    "# delete the last row\n",
    "df = df.drop(df.index[-1])\n",
    "# suffle the data\n",
    "df = df.sample(frac=1, random_state=12).reset_index(drop=True)\n",
    "\n",
    "X = df.drop([\" loan_status\"], axis=1)\n",
    "Y = df[\" loan_status\"]\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=1 - train_ratio,\n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "    random_state=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3201\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = x_train.shape\n",
    "\n",
    "# Print the size\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H=64, D_out=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Training Loss: 0.1750908616848071\n",
      "Epoch 100/1000, Validation Loss: 0.18533098921179772\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 200/1000, Training Loss: 0.13162506182447495\n",
      "Epoch 200/1000, Validation Loss: 0.15247788652777672\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 300/1000, Training Loss: 0.11690299147192727\n",
      "Epoch 300/1000, Validation Loss: 0.14303407333791257\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 400/1000, Training Loss: 0.1063396961955518\n",
      "Epoch 400/1000, Validation Loss: 0.26786382794380187\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 500/1000, Training Loss: 0.10234918854400224\n",
      "Epoch 500/1000, Validation Loss: 0.2667809769511223\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 600/1000, Training Loss: 0.09771211749837533\n",
      "Epoch 600/1000, Validation Loss: 0.2746705751866102\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 700/1000, Training Loss: 0.093744539089647\n",
      "Epoch 700/1000, Validation Loss: 0.26461437307298186\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 800/1000, Training Loss: 0.08942375083764421\n",
      "Epoch 800/1000, Validation Loss: 0.2663250438868999\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 900/1000, Training Loss: 0.08852601668560038\n",
      "Epoch 900/1000, Validation Loss: 0.2672203503549099\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 1000/1000, Training Loss: 0.08408337970282517\n",
      "Epoch 1000/1000, Validation Loss: 0.267998580634594\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net(D_in=x_train.shape[1]).to(device)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define batch size and create data loaders\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(x_train.values, y_train.values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(x_val.values, y_val.values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    i=0\n",
    "    for inputs, labels in train_loader:\n",
    "        i+=1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "     \n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print training loss for this epoch\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print validation loss for this epoch\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "        print('----------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.11444443836808205\n",
      "Test Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_dataset = CustomDataset(x_test.values, y_test.values)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_classes = (outputs >= 0.5).int()  # Convert probabilities to binary predictions\n",
    "        correct_predictions += (predicted_classes == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "print(f\"Test Accuracy: {correct_predictions / total_samples * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchTabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
