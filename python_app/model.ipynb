{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "df = pd.read_csv('loan_approval_dataset_normalized.csv')\n",
    "# delete the last row\n",
    "df = df.drop(df.index[-1])\n",
    "# suffle the data\n",
    "df = df.sample(frac=1, random_state=12).reset_index(drop=True)\n",
    "\n",
    "X = df.drop(['loan_status'], axis=1)\n",
    "Y = df['loan_status']\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    test_size=1 - train_ratio,\n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "    random_state=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3201\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = x_train.shape\n",
    "\n",
    "# Print the size\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H=64, D_out=1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Training Loss: 0.1906775922751894\n",
      "Epoch 100/1000, Validation Loss: 0.1920876309275627\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 200/1000, Training Loss: 0.16459784194754035\n",
      "Epoch 200/1000, Validation Loss: 0.16695558205246924\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 300/1000, Training Loss: 0.138257561726313\n",
      "Epoch 300/1000, Validation Loss: 0.15039304345846177\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 400/1000, Training Loss: 0.11989660800335354\n",
      "Epoch 400/1000, Validation Loss: 0.14459047093987465\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 500/1000, Training Loss: 0.10876155227017044\n",
      "Epoch 500/1000, Validation Loss: 0.13343343064188956\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 600/1000, Training Loss: 0.10172997429720222\n",
      "Epoch 600/1000, Validation Loss: 0.12708364389836788\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 700/1000, Training Loss: 0.09385873976291952\n",
      "Epoch 700/1000, Validation Loss: 0.12490362487733364\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 800/1000, Training Loss: 0.10162373062442331\n",
      "Epoch 800/1000, Validation Loss: 0.12418506592512131\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 900/1000, Training Loss: 0.08735745027661324\n",
      "Epoch 900/1000, Validation Loss: 0.12327993959188462\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 1000/1000, Training Loss: 0.08279533523554895\n",
      "Epoch 1000/1000, Validation Loss: 0.12408014461398124\n",
      "----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net(D_in=x_train.shape[1]).to(device)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define batch size and create data loaders\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(x_train.values, y_train.values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(x_val.values, y_val.values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    i=0\n",
    "    for inputs, labels in train_loader:\n",
    "        i+=1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "     \n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print training loss for this epoch\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print validation loss for this epoch\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "        print('----------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.11392818045403276\n",
      "Test Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_dataset = CustomDataset(x_test.values, y_test.values)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_classes = (outputs >= 0.5).int()  # Convert probabilities to binary predictions\n",
    "        correct_predictions += (predicted_classes == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "print(f\"Test Accuracy: {correct_predictions / total_samples * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001467578113079071\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'no_of_dependents': [0],\n",
    "    'education': [0],\n",
    "    'self_employed': [1],\n",
    "    'income_annum': [0.414141],\n",
    "    'loan_amount': [0.308861],\n",
    "    'residential_assets_value': [0.4],\n",
    "    'commercial_assets_value': [0.463333],\n",
    "    'luxury_assets_value': [0.092784],\n",
    "    'bank_asset_value': [0.113402],\n",
    "    'loan_term': [0.224490],\n",
    "    'cibil_score': [0.224490]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "inputs = torch.tensor(df.values, dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(outputs.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011481616093078628\n"
     ]
    }
   ],
   "source": [
    "# test the model for all 0 \n",
    "data = {\n",
    "    'no_of_dependents': [0],\n",
    "    'education': [0],\n",
    "    'self_employed': [1],\n",
    "    'income_annum': [0.414141],\n",
    "    'loan_amount': [0.1],\n",
    "    'residential_assets_value': [0.4],\n",
    "    'commercial_assets_value': [0.463333],\n",
    "    'luxury_assets_value': [0.092784],\n",
    "    'bank_asset_value': [0.113402],\n",
    "    'loan_term': [0.224490],\n",
    "    'cibil_score': [0.224490]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "torch_model = torch.jit.load('scripted_model.pt').to(device)\n",
    "\n",
    "df = torch.tensor(df.values, dtype=torch.float32).to(device)\n",
    "torch_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = torch_model(df)\n",
    "\n",
    "print(outputs.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)\n",
    "torch.jit.save(scripted_model, 'scripted_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchTabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
